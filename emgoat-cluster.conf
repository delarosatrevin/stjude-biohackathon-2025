#
# this is a the configuration file for cluster side information
#


#
# please apply the web hook and fill the information helow if you use slack
#
# if you use slack, please put 1; otherwise leave 0 there
#
# hook is the webhook from slack for sending the slack messages
#
[slack]
use_slack = 0
hook = https://your_http_should_be_here

#
# this section is for the lsf
#
# bjobs is the bjobs command we used inside
#
# queue_name is the query queue name for lsf
#
# bjobs format is the output options for bjobs we get the job data, please see function of parse_bjobs_output_for_alljobs
# if you have any change in the option format below
#
# we will store the lsf nodes information into the file lsf_node_data_file_name, and the file is updated every xxx minutes
# (the value is defined in lsf_nodes_data_update_time)
#
[lsf]
bjobs = bjobs -u all -json
queue_name = cryoem
bjobs_output_format = jobid stat user job_name submit_time start_time pend_time run_time time_left nreq_slot memlimit gpu_num exec_host nexec_host
bhosts_gpu_info = bhosts -gpu -w
bhosts_gpu_node_group_name = rhel8_cem
lshosts_cpu_node_list = noderome120 noderome121 noderome122 noderome123 noderome124
data_output_dir = /cryosparc/emgoat-data
node_data_file_name = lsf_nodes_infor.txt
jobs_data_file_name = lsf_jobs_infor.txt
nodes_data_update_time = 60
jobs_data_update_time = 5

#
# this section is to capture the job snapshots
#
# for a given time interval (for example like 15 minutes), this is to predict
# what is the node workload looks like based on the current job queue (pending and running jobs)
#
# job snapshot interval is how many minutes as interval we take for the snapshot. For example,
# if the value is 15 minutes, the interval is (15:30-15:45), for this time window we will generate
# one job snapshot on the nodes
#
# number_job_snapshots is the total number of job snapshots we are going to take
#
[snapshots]
job_snapshots_time_interval = 60
number_job_snapshots = 36

